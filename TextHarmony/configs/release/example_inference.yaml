
load_from: /ATA_checkpoints/896-moe-3/training_end/pytorch_model.bin

annt_path: TextHarmony/docs/examples/example.json
output_dir: TextHarmony/save_image

image_upscale: 1

data_seed: &data_seed 0
seed: 32
use_lora: True

# MODEL

model:
  llm_model_path: &tokenizer_path ./assets/lmsys/vicuna-13b-v1.3
  num_img_token: &img_len 512

  visual_tokenizer_config:
    encoder_model_path: ./assets/openai/clip-vit-large-patch14
    image_size: 896
    perceiver_config:
      num_queries: 512
      hidden_size: 768
      encoder_hidden_size: 1024
      cross_attention_frequency: 2
      num_hidden_layers: 12
      num_attention_heads: 12
      qk_normalization: True
  image_decoder_config:
    pretrained_model_name_or_path: ./assets/stabilityai/stable-diffusion-2-base
    sd_base_seed: 42
    perceiver_config:
      num_queries: 77
      hidden_size: 1024
      encoder_hidden_size: 5120
      cross_attention_frequency: 1
      num_hidden_layers: 1
      num_attention_heads: 16
      hidden_dropout_prob: 0.
      attention_probs_dropout_prob: 0.
  moe_config:
    moe_finetuning: True
    vit_lora: True
    llm_lora: True
    peft_type: moe_lora
    lora_r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
    moe_lora_num_experts: 3
    moe_gate_mode: top2_gate
    task_num: 3 # (image generation; text generation; shared)

# INFERENCE

inference:
  tokenizer_path: *tokenizer_path
  num_img_token: *img_len
  generate_mode: generate_texts
  force_gen_image_next: False
  force_replace_gen_text: False
  auto_end: False
  num_iter: 2

  transform:
    aug_type: numpy
    resolution: 896

  generation_kwargs:
    max_length: 128
    min_length: 1
    num_beams: 5
    use_nucleus_sampling: True
    repetition_penalty: 1.3
    guidance_scale: 7.5
    num_inference_steps: 30
    num_validation_images: 1

